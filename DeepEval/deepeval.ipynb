{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e09865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepeval in /home/codespace/.python/current/lib/python3.12/site-packages (3.4.6)\n",
      "Requirement already satisfied: aiohttp in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (3.12.15)\n",
      "Requirement already satisfied: anthropic in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.66.0)\n",
      "Requirement already satisfied: click<8.3.0,>=8.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (8.2.1)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.33.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.67.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.74.0)\n",
      "Requirement already satisfied: nest_asyncio in /home/codespace/.local/lib/python3.12/site-packages (from deepeval) (1.6.0)\n",
      "Requirement already satisfied: ollama in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.5.3)\n",
      "Requirement already satisfied: openai in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.105.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.24.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.36.0)\n",
      "Requirement already satisfied: portalocker in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (3.2.0)\n",
      "Requirement already satisfied: posthog<7.0.0,>=6.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (6.7.2)\n",
      "Requirement already satisfied: pyfiglet in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.0.4)\n",
      "Requirement already satisfied: pytest in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (8.4.2)\n",
      "Requirement already satisfied: pytest-asyncio in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (1.1.0)\n",
      "Requirement already satisfied: pytest-repeat in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.9.4)\n",
      "Requirement already satisfied: pytest-rerunfailures<13.0,>=12.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (12.0)\n",
      "Requirement already satisfied: pytest-xdist in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (3.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (2.32.5)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (14.1.0)\n",
      "Requirement already satisfied: sentry-sdk in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (2.36.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from deepeval) (80.9.0)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.9.0)\n",
      "Requirement already satisfied: tenacity<=10.0.0,>=8.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (9.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (4.67.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9 in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.17.3)\n",
      "Requirement already satisfied: wheel in /home/codespace/.python/current/lib/python3.12/site-packages (from deepeval) (0.45.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.40.3)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (2.11.7)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai<2.0.0,>=1.9.0->deepeval) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.9.0->deepeval) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (4.9.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.24.0->deepeval) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/codespace/.python/current/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.24.0->deepeval) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (1.36.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-proto==1.36.0->opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->deepeval) (6.32.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /home/codespace/.python/current/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->deepeval) (0.57b0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /home/codespace/.local/lib/python3.12/site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from posthog<7.0.0,>=6.3.0->deepeval) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai<2.0.0,>=1.9.0->deepeval) (0.4.1)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/codespace/.local/lib/python3.12/site-packages (from pytest-rerunfailures<13.0,>=12.0->deepeval) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->deepeval) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from rich<15.0.0,>=13.6.0->deepeval) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich<15.0.0,>=13.6.0->deepeval) (2.19.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.9.0->deepeval) (0.6.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typer<1.0.0,>=0.9->deepeval) (1.5.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.6.0->deepeval) (0.1.2)\n",
      "Requirement already satisfied: iniconfig>=1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pytest->deepeval) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from pytest->deepeval) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp->deepeval) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp->deepeval) (1.20.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from anthropic->deepeval) (0.10.0)\n",
      "Requirement already satisfied: execnet>=2.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pytest-xdist->deepeval) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cfa9c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 262\u001b[39m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluate_your_data(input_texts, actual_outputs, expected_outputs, contexts)\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    261\u001b[39m     \u001b[38;5;66;03m# Run the main example\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     results, report = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# Example of evaluating your own data\u001b[39;00m\n\u001b[32m    265\u001b[39m     \u001b[38;5;66;03m# your_inputs = [\"Your input 1\", \"Your input 2\"]\u001b[39;00m\n\u001b[32m    266\u001b[39m     \u001b[38;5;66;03m# your_outputs = [\"Your LLM output 1\", \"Your LLM output 2\"]\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# results, report = evaluate_your_data(your_inputs, your_outputs)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# Initialize evaluator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     evaluator = \u001b[43mLLMEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Sample test cases (replace with your actual data)\u001b[39;00m\n\u001b[32m    142\u001b[39m     sample_data = [\n\u001b[32m    143\u001b[39m         {\n\u001b[32m    144\u001b[39m             \u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m         }\n\u001b[32m    161\u001b[39m     ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mLLMEvaluator.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the evaluator with free metrics (no API keys needed)\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# These metrics work without API keys\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = [\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mAnswerRelevancyMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     26\u001b[39m     FaithfulnessMetric(threshold=\u001b[32m0.7\u001b[39m),\n\u001b[32m     27\u001b[39m     ContextualRelevancyMetric(threshold=\u001b[32m0.7\u001b[39m),\n\u001b[32m     28\u001b[39m     ContextualRecallMetric(threshold=\u001b[32m0.7\u001b[39m),\n\u001b[32m     29\u001b[39m     ContextualPrecisionMetric(threshold=\u001b[32m0.7\u001b[39m),\n\u001b[32m     30\u001b[39m     HallucinationMetric(threshold=\u001b[32m0.3\u001b[39m),\n\u001b[32m     31\u001b[39m     BiasMetric(threshold=\u001b[32m0.5\u001b[39m),\n\u001b[32m     32\u001b[39m     ToxicityMetric(threshold=\u001b[32m0.5\u001b[39m)\n\u001b[32m     33\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/deepeval/metrics/answer_relevancy/answer_relevancy.py:40\u001b[39m, in \u001b[36mAnswerRelevancyMetric.__init__\u001b[39m\u001b[34m(self, threshold, model, include_reason, async_mode, strict_mode, verbose_mode, evaluation_template)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m     threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     ] = AnswerRelevancyTemplate,\n\u001b[32m     38\u001b[39m ):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.threshold = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict_mode \u001b[38;5;28;01melse\u001b[39;00m threshold\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.using_native_model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m.evaluation_model = \u001b[38;5;28mself\u001b[39m.model.get_model_name()\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_reason = include_reason\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/deepeval/metrics/utils.py:469\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DeepSeekModel(model=model), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Otherwise (the model is a wrong type), we raise an error\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    473\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    474\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/deepeval/models/llms/openai_model.py:293\u001b[39m, in \u001b[36mGPTModel.__init__\u001b[39m\u001b[34m(self, model, _openai_api_key, base_url, cost_per_input_token, cost_per_output_token, temperature, generation_kwargs, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mself\u001b[39m.kwargs = kwargs\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.generation_kwargs = generation_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/deepeval/models/base_model.py:36\u001b[39m, in \u001b[36mDeepEvalBaseLLM.__init__\u001b[39m\u001b[34m(self, model_name, *args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_name = parse_model_name(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/deepeval/models/llms/openai_model.py:522\u001b[39m, in \u001b[36mGPTModel.load_model\u001b[39m\u001b[34m(self, async_mode)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, async_mode: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_mode:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_openai_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncOpenAI(\n\u001b[32m    528\u001b[39m         api_key=\u001b[38;5;28mself\u001b[39m._openai_api_key, base_url=\u001b[38;5;28mself\u001b[39m.base_url, **\u001b[38;5;28mself\u001b[39m.kwargs\n\u001b[32m    529\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.python/current/lib/python3.12/site-packages/openai/_client.py:134\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    132\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m     )\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "# DeepEval LLM Evaluation - No API Keys Required\n",
    "# First install: pip install deepeval\n",
    "\n",
    "import pandas as pd\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    HallucinationMetric,\n",
    "    BiasMetric,\n",
    "    ToxicityMetric\n",
    ")\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "class LLMEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the evaluator with free metrics (no API keys needed)\"\"\"\n",
    "        # These metrics work without API keys\n",
    "        self.metrics = [\n",
    "            AnswerRelevancyMetric(threshold=0.7),\n",
    "            FaithfulnessMetric(threshold=0.7),\n",
    "            ContextualRelevancyMetric(threshold=0.7),\n",
    "            ContextualRecallMetric(threshold=0.7),\n",
    "            ContextualPrecisionMetric(threshold=0.7),\n",
    "            HallucinationMetric(threshold=0.3),\n",
    "            BiasMetric(threshold=0.5),\n",
    "            ToxicityMetric(threshold=0.5)\n",
    "        ]\n",
    "    \n",
    "    def create_test_case(self, \n",
    "                        input_text: str, \n",
    "                        actual_output: str, \n",
    "                        expected_output: str = None,\n",
    "                        retrieval_context: List[str] = None) -> LLMTestCase:\n",
    "        \"\"\"Create a test case for evaluation\"\"\"\n",
    "        return LLMTestCase(\n",
    "            input=input_text,\n",
    "            actual_output=actual_output,\n",
    "            expected_output=expected_output,\n",
    "            retrieval_context=retrieval_context\n",
    "        )\n",
    "    \n",
    "    def evaluate_single_response(self, test_case: LLMTestCase) -> dict:\n",
    "        \"\"\"Evaluate a single response\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            try:\n",
    "                score = metric.measure(test_case)\n",
    "                results[metric.__class__.__name__] = {\n",
    "                    'score': score,\n",
    "                    'passed': metric.is_successful(),\n",
    "                    'reason': getattr(metric, 'reason', 'N/A')\n",
    "                }\n",
    "            except Exception as e:\n",
    "                results[metric.__class__.__name__] = {\n",
    "                    'score': None,\n",
    "                    'passed': False,\n",
    "                    'reason': f'Error: {str(e)}'\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_batch(self, test_cases: List[LLMTestCase]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate multiple test cases\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"Evaluating test case {i+1}/{len(test_cases)}...\")\n",
    "            \n",
    "            result = self.evaluate_single_response(test_case)\n",
    "            result['test_case_id'] = i\n",
    "            result['input'] = test_case.input\n",
    "            result['actual_output'] = test_case.actual_output\n",
    "            \n",
    "            all_results.append(result)\n",
    "        \n",
    "        return self.format_results(all_results)\n",
    "    \n",
    "    def format_results(self, results: List[dict]) -> pd.DataFrame:\n",
    "        \"\"\"Format results into a readable DataFrame\"\"\"\n",
    "        formatted_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            row = {\n",
    "                'test_case_id': result['test_case_id'],\n",
    "                'input': result['input'][:100] + '...' if len(result['input']) > 100 else result['input'],\n",
    "                'actual_output': result['actual_output'][:100] + '...' if len(result['actual_output']) > 100 else result['actual_output']\n",
    "            }\n",
    "            \n",
    "            # Add metric scores\n",
    "            for metric_name, metric_result in result.items():\n",
    "                if metric_name not in ['test_case_id', 'input', 'actual_output']:\n",
    "                    row[f'{metric_name}_score'] = metric_result.get('score')\n",
    "                    row[f'{metric_name}_passed'] = metric_result.get('passed')\n",
    "        \n",
    "        return pd.DataFrame(formatted_data)\n",
    "    \n",
    "    def save_results(self, results_df: pd.DataFrame, filename: str = 'evaluation_results.csv'):\n",
    "        \"\"\"Save results to CSV\"\"\"\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    def generate_report(self, results_df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Generate evaluation report\"\"\"\n",
    "        report = {\n",
    "            'total_test_cases': len(results_df),\n",
    "            'metrics_summary': {}\n",
    "        }\n",
    "        \n",
    "        # Calculate average scores and pass rates for each metric\n",
    "        metric_columns = [col for col in results_df.columns if col.endswith('_score')]\n",
    "        \n",
    "        for col in metric_columns:\n",
    "            metric_name = col.replace('_score', '')\n",
    "            pass_col = f'{metric_name}_passed'\n",
    "            \n",
    "            scores = results_df[col].dropna()\n",
    "            passes = results_df[pass_col].sum() if pass_col in results_df.columns else 0\n",
    "            \n",
    "            report['metrics_summary'][metric_name] = {\n",
    "                'avg_score': scores.mean() if len(scores) > 0 else 0,\n",
    "                'min_score': scores.min() if len(scores) > 0 else 0,\n",
    "                'max_score': scores.max() if len(scores) > 0 else 0,\n",
    "                'pass_rate': (passes / len(results_df)) * 100 if len(results_df) > 0 else 0,\n",
    "                'total_evaluated': len(scores)\n",
    "            }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example usage and sample data\n",
    "def main():\n",
    "    # Initialize evaluator\n",
    "    evaluator = LLMEvaluator()\n",
    "    \n",
    "    # Sample test cases (replace with your actual data)\n",
    "    sample_data = [\n",
    "        {\n",
    "            'input': 'What is the capital of France?',\n",
    "            'actual_output': 'The capital of France is Paris. Paris is located in the north-central part of France and is known for its rich history, culture, and landmarks like the Eiffel Tower.',\n",
    "            'expected_output': 'Paris',\n",
    "            'context': ['France is a country in Western Europe.', 'Paris is the capital and largest city of France.']\n",
    "        },\n",
    "        {\n",
    "            'input': 'Explain photosynthesis',\n",
    "            'actual_output': 'Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen. This process occurs in the chloroplasts of plant cells.',\n",
    "            'expected_output': 'Photosynthesis is a process used by plants to convert light energy into chemical energy.',\n",
    "            'context': ['Plants use chlorophyll to capture light energy.', 'The photosynthesis equation is 6CO2 + 6H2O + light energy â†’ C6H12O6 + 6O2.']\n",
    "        },\n",
    "        {\n",
    "            'input': 'What is machine learning?',\n",
    "            'actual_output': 'Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task.',\n",
    "            'expected_output': 'Machine learning is a method of data analysis that automates analytical model building.',\n",
    "            'context': ['Machine learning is part of AI.', 'It uses algorithms to find patterns in data.']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create test cases\n",
    "    test_cases = []\n",
    "    for data in sample_data:\n",
    "        test_case = evaluator.create_test_case(\n",
    "            input_text=data['input'],\n",
    "            actual_output=data['actual_output'],\n",
    "            expected_output=data['expected_output'],\n",
    "            retrieval_context=data['context']\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"Starting evaluation...\")\n",
    "    results_df = evaluator.evaluate_batch(test_cases)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Generate and display report\n",
    "    report = evaluator.generate_report(results_df)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    print(json.dumps(report, indent=2))\n",
    "    \n",
    "    # Save results\n",
    "    evaluator.save_results(results_df)\n",
    "    \n",
    "    return results_df, report\n",
    "\n",
    "# Custom evaluation function for your specific data\n",
    "def evaluate_your_data(input_texts: List[str], \n",
    "                      actual_outputs: List[str], \n",
    "                      expected_outputs: List[str] = None,\n",
    "                      contexts: List[List[str]] = None):\n",
    "    \"\"\"\n",
    "    Evaluate your own LLM outputs\n",
    "    \n",
    "    Args:\n",
    "        input_texts: List of input prompts\n",
    "        actual_outputs: List of LLM responses to evaluate\n",
    "        expected_outputs: List of expected/reference outputs (optional)\n",
    "        contexts: List of retrieval contexts for each input (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = LLMEvaluator()\n",
    "    test_cases = []\n",
    "    \n",
    "    for i in range(len(input_texts)):\n",
    "        expected = expected_outputs[i] if expected_outputs and i < len(expected_outputs) else None\n",
    "        context = contexts[i] if contexts and i < len(contexts) else None\n",
    "        \n",
    "        test_case = evaluator.create_test_case(\n",
    "            input_text=input_texts[i],\n",
    "            actual_output=actual_outputs[i],\n",
    "            expected_output=expected,\n",
    "            retrieval_context=context\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    \n",
    "    # Evaluate\n",
    "    results_df = evaluator.evaluate_batch(test_cases)\n",
    "    report = evaluator.generate_report(results_df)\n",
    "    \n",
    "    return results_df, report\n",
    "\n",
    "# Load data from CSV (if you have your data in a file)\n",
    "def evaluate_from_csv(csv_file_path: str, \n",
    "                     input_column: str = 'input',\n",
    "                     output_column: str = 'actual_output',\n",
    "                     expected_column: str = 'expected_output',\n",
    "                     context_column: str = 'context'):\n",
    "    \"\"\"\n",
    "    Evaluate LLM outputs from a CSV file\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    input_texts = df[input_column].tolist()\n",
    "    actual_outputs = df[output_column].tolist()\n",
    "    expected_outputs = df[expected_column].tolist() if expected_column in df.columns else None\n",
    "    \n",
    "    # Handle context column (assuming it's JSON string of list)\n",
    "    contexts = None\n",
    "    if context_column in df.columns:\n",
    "        contexts = []\n",
    "        for context_str in df[context_column]:\n",
    "            try:\n",
    "                context_list = json.loads(context_str) if pd.notna(context_str) else None\n",
    "                contexts.append(context_list)\n",
    "            except:\n",
    "                contexts.append(None)\n",
    "    \n",
    "    return evaluate_your_data(input_texts, actual_outputs, expected_outputs, contexts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main example\n",
    "    results, report = main()\n",
    "    \n",
    "    # Example of evaluating your own data\n",
    "    # your_inputs = [\"Your input 1\", \"Your input 2\"]\n",
    "    # your_outputs = [\"Your LLM output 1\", \"Your LLM output 2\"]\n",
    "    # results, report = evaluate_your_data(your_inputs, your_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f5ca39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepeval_evaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdeepeval_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_your_data\n\u001b[32m      3\u001b[39m inputs = [\u001b[33m\"\u001b[39m\u001b[33mWhat is AI?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m outputs = [\u001b[33m\"\u001b[39m\u001b[33mAI stands for Artificial Intelligence...\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'deepeval_evaluator'"
     ]
    }
   ],
   "source": [
    "from deepeval_evaluator import evaluate_your_data\n",
    "\n",
    "inputs = [\"What is AI?\"]\n",
    "outputs = [\"AI stands for Artificial Intelligence...\"]\n",
    "results, report = evaluate_your_data(inputs, outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
